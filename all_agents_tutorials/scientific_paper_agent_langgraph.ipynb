{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scientific paper agent using LangGraph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "\n",
    "\n",
    "This project implements an intelligent research assistant that helps users navigate, understand, and analyze scientific literature using LangGraph and advanced language models. By combining various academic API with sophisticated paper processing techniques, it creates a seamless experience for researchers, students, and professionals working with academic papers.\n",
    "\n",
    "> NOTE: The presented workflow is not domain specific: each step in the graph can be adapted to a different domain by simply changing the prompts.\n",
    "\n",
    "\n",
    "这个项目实现了一个智能研究助手，使用 LangGraph 和高级语言模型帮助用户导航、理解和分析科学文献。通过将各种学术 API 与复杂的论文处理技术相结合，为研究人员、学生和专业人士提供无缝的学术论文处理体验。\n",
    "\n",
    "注意：此工作流程不限于特定领域：通过简单更改提示，图中的每个步骤都可以适应不同的领域。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motivation\n",
    "\n",
    "Research literature review represents a significant time investment in R&D, with studies showing that researchers spend 30-50% of their time reading, analyzing, and synthesizing academic papers. This challenge is universal across the research community. While thorough literature review is crucial for advancing science and technology, the current process remains inefficient and time-consuming.\n",
    "\n",
    "Key challenges include:\n",
    "- Extensive time commitment (30-50% of R&D hours) dedicated to reading and processing papers\n",
    "- Inefficient search processes across fragmented database ecosystems\n",
    "- Complex task of synthesizing and connecting findings across multiple papers\n",
    "- Resource-intensive maintenance of comprehensive literature reviews\n",
    "- Ongoing effort required to stay current with new publications\n",
    "\n",
    "# 动机\n",
    "\n",
    "文献综述在研发中占用大量时间，研究表明研究人员花费30-50%的时间阅读、分析和综合学术论文。这个挑战在研究界普遍存在。虽然全面的文献综述对推进科技发展至关重要，但目前的过程仍然效率低下且耗时。\n",
    "\n",
    "主要挑战包括：\n",
    "- 大量时间投入(30-50%的研发时间)用于阅读和处理论文\n",
    "- 在分散的数据库生态系统中搜索效率低下\n",
    "- 跨多篇论文综合和连接发现的复杂任务\n",
    "- 维护全面文献综述需要大量资源\n",
    "- 需要持续努力跟进新发表的文献"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key components\n",
    "\n",
    " 1. State-Driven Workflow Engine \n",
    "    - StateGraph Architecture: Five-node system for orchestrated research \n",
    "    - Decision Making Node: Query intent analysis and routing \n",
    "    - Planning Node: Research strategy formulation\n",
    "    - Tool Execution Node: Paper retrieval and processing \n",
    "    - Judge Node: Quality validation and improvement cycles \n",
    "\n",
    "2. Paper Processing Integration \n",
    "    - Source Integration, CORE API for comprehensive paper access \n",
    "    - Document Processing, PDF content extraction, Text structure preservation \n",
    "\n",
    "3. Analysis Workflow \n",
    "    - State-aware processing pipeline \n",
    "    - Multi-step validation gates \n",
    "    - Quality-focused improvement cycles \n",
    "    - Human-in-the-loop validation options\n",
    "\n",
    "\n",
    "# 关键组件\n",
    "\n",
    "1. 状态驱动工作流引擎\n",
    "   - 状态图架构：五节点研究编排系统\n",
    "   - 决策节点：查询意图分析和路由\n",
    "   - 规划节点：研究策略制定\n",
    "   - 工具执行节点：论文检索和处理\n",
    "   - 评判节点：质量验证和改进循环\n",
    "\n",
    "2. 论文处理集成\n",
    "   - 源集成，CORE API 用于全面论文访问\n",
    "   - 文档处理，PDF 内容提取，文本结构保存\n",
    "\n",
    "3. 分析工作流\n",
    "   - 状态感知处理管道\n",
    "   - 多步验证关卡\n",
    "   - 质量导向改进循环\n",
    "   - 人机协作验证选项\n",
    "\n",
    "An overview of the workflow is shown below:\n",
    "\n",
    "![image](https://i.ibb.co/0BBzkcb/mermaid-diagram-2024-11-17-195744.png)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method details\n",
    "\n",
    "1. The system requires \n",
    "    - OpenAI API key to access GPT 4o. This model was chosen after comparing its performance with other, open-source alternatives (in particular Llama 3). However, any other LLM with tool calling capabilities can be used.\n",
    "    - CORE API key for paper retrieval. CORE is one of the larges online repositories for scientific papers, counting over 136 million papers, and offers a free API for personal use. A key can be requested [here](https://core.ac.uk/services/api#form).\n",
    "\n",
    "2. Technical Architecture: \n",
    "    - LangGraph for state orchestration.\n",
    "    - PDFplumber for document processing.\n",
    "    - Pydantic for structured data handling.\n",
    "\n",
    "> Acknowledgment: Special thanks to CORE API for enabling academic paper access.\n",
    "\n",
    "\n",
    "## 方法详情\n",
    "\n",
    "1. 系统需要\n",
    "   - OpenAI API 密钥访问 GPT 4。在与其他开源替代方案(特别是 Llama 3)比较性能后选择了此模型。但是可以使用任何具有工具调用功能的 LLM。\n",
    "   - CORE API 密钥用于论文检索。CORE 是最大的在线科学论文库之一,拥有超过1.36亿篇论文,并提供免费的个人使用 API。可以在[这里](https://core.ac.uk/services/api#form)申请密钥。\n",
    "\n",
    "2. 技术架构:\n",
    "   - LangGraph 用于状态编排\n",
    "   - PDFplumber 用于文档处理\n",
    "   - Pydantic 用于结构化数据处理\n",
    "\n",
    "> 致谢:特别感谢 CORE API 提供学术论文访问。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "This cell installs the required dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install --upgrade --quiet langchain==0.2.16 langchain-community==0.2.16 langchain-openai==0.1.23 langgraph==0.2.18 langsmith==0.1.114 pdfplumber python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell imports the required libraries and sets the environment variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import json\n",
    "import os\n",
    "import urllib3\n",
    "import time\n",
    "\n",
    "import pdfplumber\n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import display, Markdown\n",
    "from langchain_core.messages import BaseMessage, SystemMessage, ToolMessage, AIMessage\n",
    "from langchain_core.tools import BaseTool, tool\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.graph import END, StateGraph\n",
    "from langgraph.graph.state import CompiledStateGraph\n",
    "from langgraph.graph.message import add_messages\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import Annotated, ClassVar, Sequence, TypedDict\n",
    "\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "load_dotenv()\n",
    "\n",
    "# You can set your own keys here\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"sk-proj-...\"\n",
    "os.environ[\"CORE_API_KEY\"] = \"...\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompts\n",
    "\n",
    "This cell contains the prompts used in the workflow.\n",
    "\n",
    "The `agent_prompt` contains a section explaining how to use complex queries with the CORE API, enabling the agent to solve more complex tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt for the initial decision making on how to reply to the user\n",
    "decision_making_prompt = \"\"\"\n",
    "You are an experienced scientific researcher.\n",
    "Your goal is to help the user with their scientific research.\n",
    "\n",
    "Based on the user query, decide if you need to perform a research or if you can answer the question directly.\n",
    "- You should perform a research if the user query requires any supporting evidence or information.\n",
    "- You should answer the question directly only for simple conversational questions, like \"how are you?\".\n",
    "\"\"\"\n",
    "\n",
    "## 决策制定提示词\n",
    "decision_making_prompt_cn = \"\"\"\n",
    "你是一位经验丰富的科学研究员。\n",
    "你的目标是帮助用户进行科学研究。\n",
    "\n",
    "根据用户查询,决定你是需要进行研究还是可以直接回答问题。\n",
    "- 如果用户查询需要任何支持证据或信息,你应该进行研究。\n",
    "- 只有对于简单的对话性问题,如\"你好吗?\",你才应该直接回答。\n",
    "\"\"\"\n",
    "\n",
    "# Prompt to create a step by step plan to answer the user query\n",
    "planning_prompt = \"\"\"\n",
    "# IDENTITY AND PURPOSE\n",
    "\n",
    "You are an experienced scientific researcher.\n",
    "Your goal is to make a new step by step plan to help the user with their scientific research .\n",
    "\n",
    "Subtasks should not rely on any assumptions or guesses, but only rely on the information provided in the context or look up for any additional information.\n",
    "\n",
    "If any feedback is provided about a previous answer, incorportate it in your new planning.\n",
    "\n",
    "\n",
    "# TOOLS\n",
    "\n",
    "For each subtask, indicate the external tool required to complete the subtask. \n",
    "Tools can be one of the following:\n",
    "{tools}\n",
    "\"\"\"\n",
    "\n",
    "## 规划提示词\n",
    "\n",
    "planning_prompt_cn = \"\"\"\n",
    "# 身份和目的\n",
    "\n",
    "你是一位经验丰富的科学研究员。\n",
    "你的目标是制定新的分步计划来帮助用户进行科学研究。\n",
    "\n",
    "子任务不应依赖任何假设或猜测,而只应依赖上下文中提供的信息或查找任何额外信息。\n",
    "\n",
    "如果收到关于先前答案的任何反馈,请将其纳入新的规划中。\n",
    "\n",
    "# 工具\n",
    "\n",
    "对于每个子任务,指明完成子任务所需的工具。工具可以是以下之一:\n",
    "{tools}\n",
    "\"\"\"\n",
    "\n",
    "# Prompt for the agent to answer the user query\n",
    "agent_prompt = \"\"\"\n",
    "# IDENTITY AND PURPOSE\n",
    "\n",
    "You are an experienced scientific researcher. \n",
    "Your goal is to help the user with their scientific research. You have access to a set of external tools to complete your tasks.\n",
    "Follow the plan you wrote to successfully complete the task.\n",
    "\n",
    "Add extensive inline citations to support any claim made in the answer.\n",
    "\n",
    "\n",
    "# EXTERNAL KNOWLEDGE\n",
    "\n",
    "## CORE API\n",
    "\n",
    "The CORE API has a specific query language that allows you to explore a vast papers collection and perform complex queries. See the following table for a list of available operators:\n",
    "\n",
    "| Operator       | Accepted symbols         | Meaning                                                                                      |\n",
    "|---------------|-------------------------|----------------------------------------------------------------------------------------------|\n",
    "| And           | AND, +, space          | Logical binary and.                                                                           |\n",
    "| Or            | OR                     | Logical binary or.                                                                            |\n",
    "| Grouping      | (...)                  | Used to prioritise and group elements of the query.                                           |\n",
    "| Field lookup  | field_name:value       | Used to support lookup of specific fields.                                                    |\n",
    "| Range queries | fieldName(>, <,>=, <=) | For numeric and date fields, it allows to specify a range of valid values to return.         |\n",
    "| Exists queries| _exists_:fieldName     | Allows for complex queries, it returns all the items where the field specified by fieldName is not empty. |\n",
    "\n",
    "Use this table to formulate more complex queries filtering for specific papers, for example publication date/year.\n",
    "Here are the relevant fields of a paper object you can use to filter the results:\n",
    "{\n",
    "  \"authors\": [{\"name\": \"Last Name, First Name\"}],\n",
    "  \"documentType\": \"presentation\" or \"research\" or \"thesis\",\n",
    "  \"publishedDate\": \"2019-08-24T14:15:22Z\",\n",
    "  \"title\": \"Title of the paper\",\n",
    "  \"yearPublished\": \"2019\"\n",
    "}\n",
    "\n",
    "Example queries:\n",
    "- \"machine learning AND yearPublished:2023\"\n",
    "- \"maritime biology AND yearPublished>=2023 AND yearPublished<=2024\"\n",
    "- \"cancer research AND authors:Vaswani, Ashish AND authors:Bello, Irwan\"\n",
    "- \"title:Attention is all you need\"\n",
    "- \"mathematics AND _exists_:abstract\"\n",
    "\"\"\"\n",
    "\n",
    "# 代理提示词\n",
    "\n",
    "agent_prompt_cn = \"\"\"\n",
    "# 身份和目的\n",
    "\n",
    "你是一位经验丰富的科学研究员。\n",
    "你的目标是帮助用户进行科学研究。你拥有一组外部工具来完成你的任务。\n",
    "遵循你写的计划成功完成任务。\n",
    "\n",
    "在答案中添加大量内联引用以支持答案中任何提出的声明。\n",
    "\n",
    "# 外部知识\n",
    "\n",
    "## CORE API\n",
    "\n",
    "CORE API 有一个特定的查询语言,允许你探索一个庞大的论文集合并执行复杂的查询。以下是可用操作符的列表:\n",
    "\n",
    "| Operator       | Accepted symbols         | Meaning                                                                                      |\n",
    "|---------------|-------------------------|----------------------------------------------------------------------------------------------|\n",
    "| And           | AND, +, space          | Logical binary and.                                                                           |\n",
    "| Or            | OR                     | Logical binary or.                                                                            |\n",
    "| Grouping      | (...)                  | Used to prioritise and group elements of the query.                                           |\n",
    "| Field lookup  | field_name:value       | Used to support lookup of specific fields.                                                    |\n",
    "| Range queries | fieldName(>, <,>=, <=) | For numeric and date fields, it allows to specify a range of valid values to return.         |\n",
    "| Exists queries| _exists_:fieldName     | Allows for complex queries, it returns all the items where the field specified by fieldName is not empty. |\n",
    "\n",
    "\n",
    "| 操作符 | 接受的符号 | 含义 |\n",
    "|-------|------------|------|\n",
    "| 与 | AND, +, 空格 | 逻辑二元与运算 |\n",
    "| 或 | OR | 逻辑二元或运算 |\n",
    "| 分组 | (...) | 用于优先级排序和查询元素分组 |\n",
    "| 字段查找 | field_name:value | 用于支持特定字段的查找 |\n",
    "| 范围查询 | fieldName(>, <,>=, <=) | 对于数值和日期字段,允许指定有效值的范围 |\n",
    "| 存在查询 | exists:fieldName | 允许复杂查询,返回指定字段名非空的所有条目 |\n",
    "\n",
    "使用此表可以构建更复杂的查询来过滤特定论文,例如发表日期/年份。\n",
    "以下是可用于过滤结果的论文对象相关字段:\n",
    "\n",
    "{\n",
    "  \"authors\": [{\"name\": \"Last Name, First Name\"}],\n",
    "  \"documentType\": \"presentation\" or \"research\" or \"thesis\",\n",
    "  \"publishedDate\": \"2019-08-24T14:15:22Z\",\n",
    "  \"title\": \"Title of the paper\",\n",
    "  \"yearPublished\": \"2019\"\n",
    "}\n",
    "\n",
    "查询示例:\n",
    "\"机器学习 AND yearPublished:2023\"\n",
    "\"海洋生物学 AND yearPublished>=2023 AND yearPublished<=2024\"\n",
    "\"癌症研究 AND authors:Vaswani, Ashish AND authors:Bello, Irwan\"\n",
    "\"title:Attention is all you need\"\n",
    "\"数学 AND exists:abstract\"\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Prompt for the judging step to evaluate the quality of the final answer\n",
    "judge_prompt = \"\"\"\n",
    "You are an expert scientific researcher.\n",
    "Your goal is to review the final answer you provided for a specific user query.\n",
    "\n",
    "Look at the conversation history between you and the user. Based on it, you need to decide if the final answer is satisfactory or not.\n",
    "\n",
    "A good final answer should:\n",
    "- Directly answer the user query. For example, it does not answer a question about a different paper or area of research.\n",
    "- Answer extensively the request from the user.\n",
    "- Take into account any feedback given through the conversation.\n",
    "- Provide inline sources to support any claim made in the answer.\n",
    "\n",
    "In case the answer is not good enough, provide clear and concise feedback on what needs to be improved to pass the evaluation.\n",
    "\"\"\"\n",
    "\n",
    "## 评价提示词\n",
    "judge_prompt_cn = \"\"\"\n",
    "你是一位经验丰富的科学研究员。\n",
    "你的目标是审查你为特定用户查询提供的最终答案。\n",
    "\n",
    "根据你与用户的对话历史,你需要决定最终答案是否令人满意。\n",
    "\n",
    "一个好的最终答案应该:\n",
    "- 直接回答用户查询。例如,它不回答关于不同论文或研究领域的问题。\n",
    "- 广泛回答用户的请求。\n",
    "- 考虑用户通过对话提供的任何反馈。\n",
    "- 提供内联引用以支持答案中任何提出的声明。\n",
    "\n",
    "如果答案不够好,请提供清晰且简洁的反馈,说明需要改进的地方以通过评估。\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility classes and functions\n",
    "\n",
    "This cell contains the utility classes and functions used in the workflow. It includes a wrapper around the CORE API, the Pydantic models for the input and output of the nodes, and a few general-purpose functions.\n",
    "\n",
    "The `CoreAPIWrapper` class includes a retry mechanism to handle transient errors and make the workflow more robust.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class CoreAPIWrapper(BaseModel):\n",
    "#     \"\"\"CORE API的简单封装类\"\"\"\n",
    "#     # 类变量定义\n",
    "#     base_url: ClassVar[str] = \"https://api.core.ac.uk/v3\"  # CORE API的基础URL\n",
    "#     api_key: ClassVar[str] = os.environ[\"CORE_API_KEY\"]    # 从环境变量获取API密钥\n",
    "    \n",
    "#     # 实例变量定义\n",
    "#     top_k_results: int = Field(\n",
    "#         description=\"通过在Core上运行查询获得的前k个结果\",\n",
    "#         default=1\n",
    "#     )\n",
    "\n",
    "#     def _get_search_response(self, query: str) -> dict:\n",
    "#         \"\"\"\n",
    "#         私有方法：执行实际的API调用并获取搜索响应\n",
    "        \n",
    "#         参数:\n",
    "#             query (str): 搜索查询字符串\n",
    "            \n",
    "#         返回:\n",
    "#             dict: API响应的JSON数据\n",
    "            \n",
    "#         特点:\n",
    "#         - 包含重试机制,最多重试5次\n",
    "#         - 使用指数退避策略(exponential backoff)处理临时错误\n",
    "#         - 在失败时抛出异常\n",
    "#         \"\"\"\n",
    "#         http = urllib3.PoolManager()\n",
    "#         max_retries = 5    \n",
    "        \n",
    "#         for attempt in range(max_retries):\n",
    "#             # 发送GET请求到CORE API\n",
    "#             response = http.request(\n",
    "#                 'GET',\n",
    "#                 f\"{self.base_url}/search/outputs\", \n",
    "#                 headers={\"Authorization\": f\"Bearer {self.api_key}\"}, \n",
    "#                 fields={\"q\": query, \"limit\": self.top_k_results}\n",
    "#             )\n",
    "            \n",
    "#             # 检查响应状态\n",
    "#             if 200 <= response.status < 300:\n",
    "#                 return response.json()\n",
    "#             elif attempt < max_retries - 1:\n",
    "#                 # 如果失败但还有重试机会,等待一段时间后重试\n",
    "#                 # 2**(attempt+2) 表示等待时间随重试次数指数增加\n",
    "#                 time.sleep(2 ** (attempt + 2))\n",
    "#             else:\n",
    "#                 # 所有重试都失败时抛出异常\n",
    "#                 raise Exception(f\"从CORE API获得非2xx响应: {response.status} {response.data}\")\n",
    "\n",
    "#     def search(self, query: str) -> str:\n",
    "#         \"\"\"\n",
    "#         公开方法：执行论文搜索并格式化结果\n",
    "        \n",
    "#         参数:\n",
    "#             query (str): 搜索查询字符串\n",
    "            \n",
    "#         返回:\n",
    "#             str: 格式化的搜索结果字符串\n",
    "            \n",
    "#         功能:\n",
    "#         - 调用API搜索论文\n",
    "#         - 提取关键信息(ID,标题,发布日期,作者等)\n",
    "#         - 将结果格式化为易读的字符串\n",
    "#         \"\"\"\n",
    "#         response = self._get_search_response(query)\n",
    "#         results = response.get(\"results\", [])\n",
    "        \n",
    "#         # 处理无结果的情况\n",
    "#         if not results:\n",
    "#             return \"No relevant results were found\"\n",
    "\n",
    "#         # 格式化搜索结果\n",
    "#         docs = []\n",
    "#         for result in results:\n",
    "#             # 获取发布日期(优先使用publishedDate,否则使用yearPublished)\n",
    "#             published_date_str = result.get('publishedDate') or result.get('yearPublished', '')\n",
    "#             # 将作者列表转换为字符串\n",
    "#             authors_str = ' and '.join([item['name'] for item in result.get('authors', [])])\n",
    "            \n",
    "#             # 构建每篇论文的信息字符串\n",
    "#             docs.append((\n",
    "#                 f\"* ID: {result.get('id', '')},\\n\"\n",
    "#                 f\"* Title: {result.get('title', '')},\\n\"\n",
    "#                 f\"* Published Date: {published_date_str},\\n\"\n",
    "#                 f\"* Authors: {authors_str},\\n\"\n",
    "#                 f\"* Abstract: {result.get('abstract', '')},\\n\"\n",
    "#                 f\"* Paper URLs: {result.get('sourceFulltextUrls') or result.get('downloadUrl', '')}\"\n",
    "#             ))\n",
    "        \n",
    "#         # 用分隔符连接所有论文信息\n",
    "#         return \"\\n-----\\n\".join(docs)\n",
    "\n",
    "class CoreAPIWrapper(BaseModel):\n",
    "    \"\"\"Simple wrapper around the CORE API.\"\"\"\n",
    "    base_url: ClassVar[str] = \"https://api.core.ac.uk/v3\"\n",
    "    api_key: ClassVar[str] = os.environ[\"CORE_API_KEY\"]\n",
    "\n",
    "    top_k_results: int = Field(description = \"Top k results obtained by running a query on Core\", default = 1)\n",
    "\n",
    "    def _get_search_response(self, query: str) -> dict:\n",
    "        http = urllib3.PoolManager()\n",
    "\n",
    "        # Retry mechanism to handle transient errors\n",
    "        max_retries = 5    \n",
    "        for attempt in range(max_retries):\n",
    "            response = http.request(\n",
    "                'GET',\n",
    "                f\"{self.base_url}/search/outputs\", \n",
    "                headers={\"Authorization\": f\"Bearer {self.api_key}\"}, \n",
    "                fields={\"q\": query, \"limit\": self.top_k_results}\n",
    "            )\n",
    "            if 200 <= response.status < 300:\n",
    "                return response.json()\n",
    "            elif attempt < max_retries - 1:\n",
    "                time.sleep(2 ** (attempt + 2))\n",
    "            else:\n",
    "                raise Exception(f\"Got non 2xx response from CORE API: {response.status} {response.data}\")\n",
    "\n",
    "    def search(self, query: str) -> str:\n",
    "        response = self._get_search_response(query)\n",
    "        results = response.get(\"results\", [])\n",
    "        if not results:\n",
    "            return \"No relevant results were found\"\n",
    "\n",
    "        # Format the results in a string\n",
    "        docs = []\n",
    "        for result in results:\n",
    "            published_date_str = result.get('publishedDate') or result.get('yearPublished', '')\n",
    "            authors_str = ' and '.join([item['name'] for item in result.get('authors', [])])\n",
    "            docs.append((\n",
    "                f\"* ID: {result.get('id', '')},\\n\"\n",
    "                f\"* Title: {result.get('title', '')},\\n\"\n",
    "                f\"* Published Date: {published_date_str},\\n\"\n",
    "                f\"* Authors: {authors_str},\\n\"\n",
    "                f\"* Abstract: {result.get('abstract', '')},\\n\"\n",
    "                f\"* Paper URLs: {result.get('sourceFulltextUrls') or result.get('downloadUrl', '')}\"\n",
    "            ))\n",
    "        return \"\\n-----\\n\".join(docs)\n",
    "\n",
    "\n",
    "# class SearchPapersInput(BaseModel):\n",
    "#     \"\"\"\n",
    "#     论文搜索输入参数的模型类\n",
    "    \n",
    "#     属性:\n",
    "#         query (str): 搜索查询字符串\n",
    "#         max_papers (int): 返回的最大论文数量,默认为1,范围1-10\n",
    "#     \"\"\"\n",
    "#     query: str = Field(description=\"要在选定存档中搜索的查询\")\n",
    "#     max_papers: int = Field(\n",
    "#         description=\"要返回的最大论文数。默认为1,如果需要进行更全面的搜索,最多可增加到10篇。\",\n",
    "#         default=1, \n",
    "#         ge=1,  # 大于等于1\n",
    "#         le=10  # 小于等于10\n",
    "#     )\n",
    "\n",
    "class SearchPapersInput(BaseModel):\n",
    "    \"\"\"Input object to search papers with the CORE API.\"\"\"\n",
    "    query: str = Field(description=\"The query to search for on the selected archive.\")\n",
    "    max_papers: int = Field(description=\"The maximum number of papers to return. It's default to 1, but you can increase it up to 10 in case you need to perform a more comprehensive search.\", default=1, ge=1, le=10)\n",
    "\n",
    "# class DecisionMakingOutput(BaseModel):\n",
    "#     \"\"\"\n",
    "#     决策制定节点的输出模型类\n",
    "    \n",
    "#     属性:\n",
    "#         requires_research (bool): 用户查询是否需要研究\n",
    "#         answer (str | None): 对用户查询的回答。如果需要研究则为None,否则为直接回答\n",
    "#     \"\"\"\n",
    "#     requires_research: bool = Field(description=\"用户查询是否需要研究\")\n",
    "#     answer: str | None = Field(default=None, description=\"对用户查询的回答\")\n",
    "\n",
    "\n",
    "class DecisionMakingOutput(BaseModel):\n",
    "    \"\"\"Output object of the decision making node.\"\"\"\n",
    "    requires_research: bool = Field(description=\"Whether the user query requires research or not.\")\n",
    "    answer: str | None = Field(default=None, description=\"The answer to the user query. It should be None if the user query requires research, otherwise it should be a direct answer to the user query.\")\n",
    "\n",
    "# class JudgeOutput(BaseModel):\n",
    "#     \"\"\"\n",
    "#     评判节点的输出模型类\n",
    "    \n",
    "#     属性:\n",
    "#         is_good_answer (bool): 回答是否良好\n",
    "#         feedback (str | None): 如果回答不好,提供详细反馈。如果回答好,则为None\n",
    "#     \"\"\"\n",
    "#     is_good_answer: bool = Field(description=\"回答是否良好\")\n",
    "#     feedback: str | None = Field(default=None, description=\"如果回答不好的详细反馈\")\n",
    "\n",
    "\n",
    "class JudgeOutput(BaseModel):\n",
    "    \"\"\"Output object of the judge node.\"\"\"\n",
    "    is_good_answer: bool = Field(description=\"Whether the answer is good or not.\")\n",
    "    feedback: str | None = Field(default=None, description=\"Detailed feedback about why the answer is not good. It should be None if the answer is good.\")\n",
    "\n",
    "# def format_tools_description(tools: list[BaseTool]) -> str:\n",
    "#     \"\"\"\n",
    "#     格式化工具描述\n",
    "    \n",
    "#     参数:\n",
    "#         tools (list[BaseTool]): 工具列表\n",
    "        \n",
    "#     返回:\n",
    "#         str: 格式化后的工具描述字符串\n",
    "#     \"\"\"\n",
    "#     return \"\\n\\n\".join([f\"- {tool.name}: {tool.description}\\n Input arguments: {tool.args}\" for tool in tools])\n",
    "\n",
    "\n",
    "def format_tools_description(tools: list[BaseTool]) -> str:\n",
    "    return \"\\n\\n\".join([f\"- {tool.name}: {tool.description}\\n Input arguments: {tool.args}\" for tool in tools])\n",
    "\n",
    "# async def print_stream(app: CompiledStateGraph, input: str) -> BaseMessage | None:\n",
    "#     \"\"\"\n",
    "#     异步打印流式输出\n",
    "    \n",
    "#     参数:\n",
    "#         app (CompiledStateGraph): 编译后的状态图\n",
    "#         input (str): 输入字符串\n",
    "        \n",
    "#     返回:\n",
    "#         BaseMessage | None: 最后一条消息或None\n",
    "        \n",
    "#     功能:\n",
    "#     - 显示新研究运行的标题\n",
    "#     - 显示输入内容\n",
    "#     - 流式显示结果\n",
    "#     - 返回最后一条消息\n",
    "#     \"\"\"\n",
    "#     display(Markdown(\"## New research running\"))\n",
    "#     display(Markdown(f\"### Input:\\n\\n{input}\\n\\n\"))\n",
    "#     display(Markdown(\"### Stream:\\n\\n\"))\n",
    "\n",
    "#     # 流式输出结果\n",
    "#     all_messages = []\n",
    "#     async for chunk in app.astream({\"messages\": [input]}, stream_mode=\"updates\"):\n",
    "#         for updates in chunk.values():\n",
    "#             if messages := updates.get(\"messages\"):\n",
    "#                 all_messages.extend(messages)\n",
    "#                 for message in messages:\n",
    "#                     message.pretty_print()\n",
    "#                     print(\"\\n\\n\")\n",
    " \n",
    "#     # 返回最后一条消息(如果有)\n",
    "#     if not all_messages:\n",
    "#         return None\n",
    "#     return all_messages[-1]\n",
    "\n",
    "async def print_stream(app: CompiledStateGraph, input: str) -> BaseMessage | None:\n",
    "    display(Markdown(\"## New research running\"))\n",
    "    display(Markdown(f\"### Input:\\n\\n{input}\\n\\n\"))\n",
    "    display(Markdown(\"### Stream:\\n\\n\"))\n",
    "\n",
    "    # Stream the results \n",
    "    all_messages = []\n",
    "    async for chunk in app.astream({\"messages\": [input]}, stream_mode=\"updates\"):\n",
    "        for updates in chunk.values():\n",
    "            if messages := updates.get(\"messages\"):\n",
    "                all_messages.extend(messages)\n",
    "                for message in messages:\n",
    "                    message.pretty_print()\n",
    "                    print(\"\\n\\n\")\n",
    " \n",
    "    # Return the last message if any\n",
    "    if not all_messages:\n",
    "        return None\n",
    "    return all_messages[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent state\n",
    "\n",
    "This cell defines the agent state, which contains the following information:\n",
    "- `requires_research`: Whether the user query requires research or not.\n",
    "- `num_feedback_requests`: The number of times the LLM asked for feedback.\n",
    "- `is_good_answer`: Whether the LLM's final answer is good or not.\n",
    "- `messages`: The conversation history between the user and the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class AgentState(TypedDict):\n",
    "#     \"\"\"\n",
    "#     代理在论文研究过程中的状态类\n",
    "    \n",
    "#     属性:\n",
    "#         requires_research (bool): 是否需要研究,默认False\n",
    "#         num_feedback_requests (int): LLM请求反馈的次数,默认0\n",
    "#         is_good_answer (bool): 最终答案是否良好,默认False\n",
    "#         messages (Sequence[BaseMessage]): 用户和LLM之间的对话历史\n",
    "#     \"\"\"\n",
    "#     requires_research: bool = False\n",
    "#     num_feedback_requests: int = 0\n",
    "#     is_good_answer: bool = False\n",
    "#     messages: Annotated[Sequence[BaseMessage], add_messages]\n",
    "\n",
    "# AgentState类型定义\n",
    "class AgentState(TypedDict):\n",
    "    \"\"\"The state of the agent during the paper research process.\"\"\"\n",
    "    requires_research: bool = False\n",
    "    num_feedback_requests: int = 0\n",
    "    is_good_answer: bool = False\n",
    "    messages: Annotated[Sequence[BaseMessage], add_messages]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent tools\n",
    "\n",
    "This cell defines the tools available to the agent. The toolkit contains a tool to search for scientific papers using the CORE API, a tool to download a scientific paper from a given URL, and a tool to ask for human feedback.\n",
    "\n",
    "To make the paper download more robust, the tool includes a retry mechanism, similar to the one used for the CORE API, as well as a mock browser header to avoid 403 errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @tool(\"search-papers\", args_schema=SearchPapersInput)\n",
    "# def search_papers(query: str, max_papers: int = 1) -> str:\n",
    "#     \"\"\"\n",
    "#     使用CORE API搜索科学论文的工具函数\n",
    "    \n",
    "#     参数:\n",
    "#         query (str): 搜索查询字符串\n",
    "#         max_papers (int): 要返回的最大论文数量,默认为1\n",
    "        \n",
    "#     返回:\n",
    "#         str: 找到的相关论文信息列表\n",
    "        \n",
    "#     示例:\n",
    "#         输入: {\"query\": \"Attention is all you need\", \"max_papers\": 1}\n",
    "#     \"\"\"\n",
    "#     try:\n",
    "#         return CoreAPIWrapper(top_k_results=max_papers).search(query)\n",
    "#     except Exception as e:\n",
    "#         return f\"Error performing paper search: {e}\"\n",
    "\n",
    "\n",
    "@tool(\"search-papers\", args_schema=SearchPapersInput)\n",
    "def search_papers(query: str, max_papers: int = 1) -> str:\n",
    "    \"\"\"Search for scientific papers using the CORE API.\n",
    "\n",
    "    Example:\n",
    "    {\"query\": \"Attention is all you need\", \"max_papers\": 1}\n",
    "\n",
    "    Returns:\n",
    "        A list of the relevant papers found with the corresponding relevant information.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return CoreAPIWrapper(top_k_results=max_papers).search(query)\n",
    "    except Exception as e:\n",
    "        return f\"Error performing paper search: {e}\"\n",
    "\n",
    "# @tool(\"download-paper\")\n",
    "# def download_paper(url: str) -> str:\n",
    "#     \"\"\"\n",
    "#     从给定URL下载科学论文的工具函数\n",
    "    \n",
    "#     参数:\n",
    "#         url (str): 论文的URL地址\n",
    "        \n",
    "#     返回:\n",
    "#         str: 论文内容文本\n",
    "        \n",
    "#     特点:\n",
    "#     - 包含重试机制\n",
    "#     - 模拟浏览器请求头以避免403错误\n",
    "#     - 使用pdfplumber提取PDF文本\n",
    "#     \"\"\"\n",
    "#     try:        \n",
    "#         http = urllib3.PoolManager(\n",
    "#             cert_reqs='CERT_NONE',  # 禁用证书验证\n",
    "#         )\n",
    "        \n",
    "#         # 模拟浏览器请求头以避免被拦截\n",
    "#         headers = {\n",
    "#             'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
    "#             'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n",
    "#             'Accept-Language': 'en-US,en;q=0.5',\n",
    "#             'Accept-Encoding': 'gzip, deflate, br',\n",
    "#             'Connection': 'keep-alive',\n",
    "#         }\n",
    "        \n",
    "#         # 重试机制\n",
    "#         max_retries = 5\n",
    "#         for attempt in range(max_retries):\n",
    "#             response = http.request('GET', url, headers=headers)\n",
    "#             if 200 <= response.status < 300:\n",
    "#                 # 将响应内容转换为PDF文件对象\n",
    "#                 pdf_file = io.BytesIO(response.data)\n",
    "#                 # 使用pdfplumber提取文本\n",
    "#                 with pdfplumber.open(pdf_file) as pdf:\n",
    "#                     text = \"\"\n",
    "#                     for page in pdf.pages:\n",
    "#                         text += page.extract_text() + \"\\n\"\n",
    "#                 return text\n",
    "#             elif attempt < max_retries - 1:\n",
    "#                 # 使用指数退避策略\n",
    "#                 time.sleep(2 ** (attempt + 2))\n",
    "#             else:\n",
    "#                 raise Exception(f\"Got non 2xx when downloading paper: {response.status_code} {response.text}\")\n",
    "#     except Exception as e:\n",
    "#         return f\"Error downloading paper: {e}\"\n",
    "\n",
    "@tool(\"download-paper\")\n",
    "def download_paper(url: str) -> str:\n",
    "    \"\"\"Download a specific scientific paper from a given URL.\n",
    "\n",
    "    Example:\n",
    "    {\"url\": \"https://sample.pdf\"}\n",
    "\n",
    "    Returns:\n",
    "        The paper content.\n",
    "    \"\"\"\n",
    "    try:        \n",
    "        http = urllib3.PoolManager(\n",
    "            cert_reqs='CERT_NONE',\n",
    "        )\n",
    "        \n",
    "        # Mock browser headers to avoid 403 error\n",
    "        headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
    "            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n",
    "            'Accept-Language': 'en-US,en;q=0.5',\n",
    "            'Accept-Encoding': 'gzip, deflate, br',\n",
    "            'Connection': 'keep-alive',\n",
    "        }\n",
    "        max_retries = 5\n",
    "        for attempt in range(max_retries):\n",
    "            response = http.request('GET', url, headers=headers)\n",
    "            if 200 <= response.status < 300:\n",
    "                pdf_file = io.BytesIO(response.data)\n",
    "                with pdfplumber.open(pdf_file) as pdf:\n",
    "                    text = \"\"\n",
    "                    for page in pdf.pages:\n",
    "                        text += page.extract_text() + \"\\n\"\n",
    "                return text\n",
    "            elif attempt < max_retries - 1:\n",
    "                time.sleep(2 ** (attempt + 2))\n",
    "            else:\n",
    "                raise Exception(f\"Got non 2xx when downloading paper: {response.status_code} {response.text}\")\n",
    "    except Exception as e:\n",
    "        return f\"Error downloading paper: {e}\"\n",
    "\n",
    "# @tool(\"ask-human-feedback\")\n",
    "# def ask_human_feedback(question: str) -> str:\n",
    "#     \"\"\"\n",
    "#     请求人类反馈的工具函数\n",
    "    \n",
    "#     参数:\n",
    "#         question (str): 要询问的问题\n",
    "        \n",
    "#     返回:\n",
    "#         str: 人类用户的输入反馈\n",
    "#     \"\"\"\n",
    "#     return input(question)\n",
    "\n",
    "@tool(\"ask-human-feedback\")\n",
    "def ask_human_feedback(question: str) -> str:\n",
    "    \"\"\"Ask for human feedback. You should call this tool when encountering unexpected errors.\"\"\"\n",
    "    return input(question)\n",
    "\n",
    "tools = [search_papers, download_paper, ask_human_feedback]\n",
    "tools_dict = {tool.name: tool for tool in tools}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workflow nodes\n",
    "\n",
    "This cell defines the nodes of the workflow. Note how the `judge_node` is configured to end the execution if the LLM failed to provide a good answer twice to keep latency acceptable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLMs\n",
    "\n",
    "\"\"\"\n",
    "- with_structured_output() 用于确保LLM输出符合预定义的数据结构\n",
    "- DecisionMakingOutput 和 JudgeOutput 是Pydantic模型,定义了预期的输出格式\n",
    "- 这种配置适用于需要固定格式输出的节点,如:\n",
    "  - 决策节点(是否需要研究)\n",
    "  - 评判节点(答案质量评估)\n",
    "- 工具绑定配置:\n",
    "  - bind_tools() 使LLM能够访问和使用外部工具\n",
    "  - 这个配置让LLM可以:\n",
    "    - 调用论文搜索API\n",
    "    - 下载PDF文件\n",
    "    - 请求人类反馈\n",
    "  - 适用于需要执行具体操作的代理节点\n",
    "- 总结:\n",
    "  - with_structured_output: 用于需要规范化输出格式的场景\n",
    "  - bind_tools: 用于需要执行外部操作的场景\n",
    "  - 这种分工让每个LLM实例都专注于特定任务,提高了系统的模块化程度和可维护性\n",
    "\"\"\"\n",
    "\n",
    "base_llm = ChatOpenAI(model=\"gpt-4o\", temperature=0.0)\n",
    "decision_making_llm = base_llm.with_structured_output(DecisionMakingOutput)\n",
    "agent_llm = base_llm.bind_tools(tools)\n",
    "judge_llm = base_llm.with_structured_output(JudgeOutput)\n",
    "\n",
    "# # 工作流节点定义\n",
    "# def decision_making_node(state: AgentState):\n",
    "#     \"\"\"\n",
    "#     工作流的入口节点\n",
    "    \n",
    "#     功能:\n",
    "#     - 基于用户查询决定是直接回答还是执行完整研究\n",
    "#     - 如果可以直接回答,生成回答\n",
    "#     - 如果需要研究,将工作流路由到规划节点\n",
    "    \n",
    "#     参数:\n",
    "#         state (AgentState): 当前代理状态\n",
    "        \n",
    "#     返回:\n",
    "#         dict: 包含决策结果和可能的回答\n",
    "#     \"\"\"\n",
    "#     system_prompt = SystemMessage(content=decision_making_prompt)\n",
    "#     response: DecisionMakingOutput = decision_making_llm.invoke([system_prompt] + state[\"messages\"])\n",
    "#     output = {\"requires_research\": response.requires_research}\n",
    "#     if response.answer:\n",
    "#         output[\"messages\"] = [AIMessage(content=response.answer)]\n",
    "#     return output\n",
    "\n",
    "# Decision making node\n",
    "def decision_making_node(state: AgentState):\n",
    "    \"\"\"Entry point of the workflow. Based on the user query, the model can either respond directly or perform a full research, routing the workflow to the planning node\"\"\"\n",
    "    system_prompt = SystemMessage(content=decision_making_prompt)\n",
    "    response: DecisionMakingOutput = decision_making_llm.invoke([system_prompt] + state[\"messages\"])\n",
    "    output = {\"requires_research\": response.requires_research}\n",
    "    if response.answer:\n",
    "        output[\"messages\"] = [AIMessage(content=response.answer)]\n",
    "    return output\n",
    "  \n",
    "# def router(state: AgentState):\n",
    "#     \"\"\"\n",
    "#     路由函数,将用户查询定向到工作流的适当分支\n",
    "    \n",
    "#     参数:\n",
    "#         state (AgentState): 当前代理状态\n",
    "        \n",
    "#     返回:\n",
    "#         str: 下一个节点的名称(\"planning\"或\"end\")\n",
    "#     \"\"\"\n",
    "#     if state[\"requires_research\"]:\n",
    "#         return \"planning\"\n",
    "#     else:\n",
    "#         return \"end\"\n",
    "\n",
    "# Task router function\n",
    "def router(state: AgentState):\n",
    "    \"\"\"Router directing the user query to the appropriate branch of the workflow.\"\"\"\n",
    "    if state[\"requires_research\"]:\n",
    "        return \"planning\"\n",
    "    else:\n",
    "        return \"end\"\n",
    "\n",
    "\n",
    "# def planning_node(state: AgentState):\n",
    "#     \"\"\"\n",
    "#     规划节点,创建回答用户查询的分步计划\n",
    "    \n",
    "#     参数:\n",
    "#         state (AgentState): 当前代理状态\n",
    "        \n",
    "#     返回:\n",
    "#         dict: 包含计划的消息\n",
    "#     \"\"\"\n",
    "#     system_prompt = SystemMessage(content=planning_prompt.format(tools=format_tools_description(tools)))\n",
    "#     response = base_llm.invoke([system_prompt] + state[\"messages\"])\n",
    "#     return {\"messages\": [response]}\n",
    "\n",
    "# Planning node\n",
    "def planning_node(state: AgentState):\n",
    "    \"\"\"Planning node that creates a step by step plan to answer the user query.\"\"\"\n",
    "    system_prompt = SystemMessage(content=planning_prompt.format(tools=format_tools_description(tools)))\n",
    "    response = base_llm.invoke([system_prompt] + state[\"messages\"])\n",
    "    return {\"messages\": [response]}\n",
    "  \n",
    "  \n",
    "# def tools_node(state: AgentState):\n",
    "#     \"\"\"\n",
    "#     工具调用节点,根据计划执行工具\n",
    "    \n",
    "#     参数:\n",
    "#         state (AgentState): 当前代理状态\n",
    "        \n",
    "#     返回:\n",
    "#         dict: 包含工具执行结果的消息\n",
    "#     \"\"\"\n",
    "#     outputs = []\n",
    "#     for tool_call in state[\"messages\"][-1].tool_calls:\n",
    "#         tool_result = tools_dict[tool_call[\"name\"]].invoke(tool_call[\"args\"])\n",
    "#         outputs.append(\n",
    "#             ToolMessage(\n",
    "#                 content=json.dumps(tool_result),\n",
    "#                 name=tool_call[\"name\"],\n",
    "#                 tool_call_id=tool_call[\"id\"],\n",
    "#             )\n",
    "#         )\n",
    "#     return {\"messages\": outputs}\n",
    "\n",
    "# Tool call node\n",
    "def tools_node(state: AgentState):\n",
    "    \"\"\"Tool call node that executes the tools based on the plan.\"\"\"\n",
    "    outputs = []\n",
    "    for tool_call in state[\"messages\"][-1].tool_calls:\n",
    "        tool_result = tools_dict[tool_call[\"name\"]].invoke(tool_call[\"args\"])\n",
    "        outputs.append(\n",
    "            ToolMessage(\n",
    "                content=json.dumps(tool_result),\n",
    "                name=tool_call[\"name\"],\n",
    "                tool_call_id=tool_call[\"id\"],\n",
    "            )\n",
    "        )\n",
    "    return {\"messages\": outputs}\n",
    "  \n",
    "# def agent_node(state: AgentState):\n",
    "#     \"\"\"\n",
    "#     代理节点,使用LLM和工具回答用户查询\n",
    "    \n",
    "#     参数:\n",
    "#         state (AgentState): 当前代理状态,包含对话历史和上下文\n",
    "        \n",
    "#     返回:\n",
    "#         dict: 包含代理响应的消息\n",
    "        \n",
    "#     功能:\n",
    "#     - 使用系统提示词设置代理行为\n",
    "#     - 调用配备工具的LLM处理查询\n",
    "#     - 生成结构化响应\n",
    "#     \"\"\"\n",
    "#     system_prompt = SystemMessage(content=agent_prompt)\n",
    "#     response = agent_llm.invoke([system_prompt] + state[\"messages\"])\n",
    "#     return {\"messages\": [response]}\n",
    "\n",
    "\n",
    "# Agent call node\n",
    "def agent_node(state: AgentState):\n",
    "    \"\"\"Agent call node that uses the LLM with tools to answer the user query.\"\"\"\n",
    "    system_prompt = SystemMessage(content=agent_prompt)\n",
    "    response = agent_llm.invoke([system_prompt] + state[\"messages\"])\n",
    "    return {\"messages\": [response]}\n",
    "  \n",
    "  \n",
    "# def should_continue(state: AgentState):\n",
    "#     \"\"\"\n",
    "#     检查代理是否应该继续执行或结束\n",
    "    \n",
    "#     参数:\n",
    "#         state (AgentState): 当前代理状态\n",
    "        \n",
    "#     返回:\n",
    "#         str: \"continue\"表示继续执行,\"end\"表示结束\n",
    "        \n",
    "#     逻辑:\n",
    "#     - 检查最后一条消息是否包含工具调用\n",
    "#     - 有工具调用则继续执行\n",
    "#     - 无工具调用则结束流程\n",
    "#     \"\"\"\n",
    "#     messages = state[\"messages\"]\n",
    "#     last_message = messages[-1]\n",
    "\n",
    "#     # 如果没有工具调用则结束执行\n",
    "#     if last_message.tool_calls:\n",
    "#         return \"continue\"\n",
    "#     else:\n",
    "#         return \"end\"\n",
    "\n",
    "# Should continue function\n",
    "def should_continue(state: AgentState):\n",
    "    \"\"\"Check if the agent should continue or end.\"\"\"\n",
    "    messages = state[\"messages\"]\n",
    "    last_message = messages[-1]\n",
    "\n",
    "    # End execution if there are no tool calls\n",
    "    if last_message.tool_calls:\n",
    "        return \"continue\"\n",
    "    else:\n",
    "        return \"end\"\n",
    "      \n",
    "# def judge_node(state: AgentState):\n",
    "#     \"\"\"\n",
    "#     评判节点,让LLM评估其最终答案的质量\n",
    "    \n",
    "#     参数:\n",
    "#         state (AgentState): 当前代理状态\n",
    "        \n",
    "#     返回:\n",
    "#         dict: 包含评判结果和可能的反馈\n",
    "        \n",
    "#     特点:\n",
    "#     - 如果LLM两次未能提供好的答案则结束执行\n",
    "#     - 使用专门的评判提示词指导评估\n",
    "#     - 可以提供具体的改进反馈\n",
    "#     \"\"\"\n",
    "#     # 如果LLM两次未能提供好的答案则结束执行\n",
    "#     num_feedback_requests = state.get(\"num_feedback_requests\", 0)\n",
    "#     if num_feedback_requests >= 2:\n",
    "#         return {\"is_good_answer\": True}\n",
    "\n",
    "#     system_prompt = SystemMessage(content=judge_prompt)\n",
    "#     response: JudgeOutput = judge_llm.invoke([system_prompt] + state[\"messages\"])\n",
    "#     output = {\n",
    "#         \"is_good_answer\": response.is_good_answer,\n",
    "#         \"num_feedback_requests\": num_feedback_requests + 1\n",
    "#     }\n",
    "#     if response.feedback:\n",
    "#         output[\"messages\"] = [AIMessage(content=response.feedback)]\n",
    "#     return output\n",
    "\n",
    "# Judge node\n",
    "def judge_node(state: AgentState):\n",
    "    \"\"\"Node to let the LLM judge the quality of its own final answer.\"\"\"\n",
    "    # End execution if the LLM failed to provide a good answer twice.\n",
    "    num_feedback_requests = state.get(\"num_feedback_requests\", 0)\n",
    "    if num_feedback_requests >= 2:\n",
    "        return {\"is_good_answer\": True}\n",
    "\n",
    "    system_prompt = SystemMessage(content=judge_prompt)\n",
    "    response: JudgeOutput = judge_llm.invoke([system_prompt] + state[\"messages\"])\n",
    "    output = {\n",
    "        \"is_good_answer\": response.is_good_answer,\n",
    "        \"num_feedback_requests\": num_feedback_requests + 1\n",
    "    }\n",
    "    if response.feedback:\n",
    "        output[\"messages\"] = [AIMessage(content=response.feedback)]\n",
    "    return output\n",
    "  \n",
    "# def final_answer_router(state: AgentState):\n",
    "#     \"\"\"\n",
    "#     最终答案路由器,决定是结束工作流还是改进答案\n",
    "    \n",
    "#     参数:\n",
    "#         state (AgentState): 当前代理状态\n",
    "        \n",
    "#     返回:\n",
    "#         str: \"end\"表示结束工作流,\"planning\"表示返回规划阶段改进答案\n",
    "#     \"\"\"\n",
    "#     if state[\"is_good_answer\"]:\n",
    "#         return \"end\"\n",
    "#     else:\n",
    "#         return \"planning\"\n",
    "\n",
    "# Final answer router function\n",
    "def final_answer_router(state: AgentState):\n",
    "    \"\"\"Router to end the workflow or improve the answer.\"\"\"\n",
    "    if state[\"is_good_answer\"]:\n",
    "        return \"end\"\n",
    "    else:\n",
    "        return \"planning\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workflow definition\n",
    "\n",
    "This cell defines the workflow using LangGraph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the StateGraph\n",
    "workflow = StateGraph(AgentState)\n",
    "\n",
    "# Add nodes to the graph\n",
    "# 向图中添加节点\n",
    "workflow.add_node(\"decision_making\", decision_making_node)  # 决策制定节点\n",
    "workflow.add_node(\"planning\", planning_node)               # 规划节点\n",
    "workflow.add_node(\"tools\", tools_node)                    # 工具节点\n",
    "workflow.add_node(\"agent\", agent_node)                    # 代理节点\n",
    "workflow.add_node(\"judge\", judge_node)                    # 评判节点\n",
    "\n",
    "\n",
    "# Set the entry point of the graph\n",
    "# 设置图的入口点\n",
    "workflow.set_entry_point(\"decision_making\")\n",
    "\n",
    "# Add edges between nodes\n",
    "# 添加节点之间的边\n",
    "workflow.add_conditional_edges(\n",
    "    \"decision_making\",\n",
    "    router,\n",
    "    {\n",
    "        \"planning\": \"planning\",\n",
    "        \"end\": END,\n",
    "    }\n",
    ")\n",
    "\n",
    "# 添加基本边\n",
    "workflow.add_edge(\"planning\", \"agent\")\n",
    "workflow.add_edge(\"tools\", \"agent\")\n",
    "\n",
    "# 添加条件边\n",
    "workflow.add_conditional_edges(\n",
    "    \"agent\",\n",
    "    should_continue,\n",
    "    {\n",
    "        \"continue\": \"tools\",\n",
    "        \"end\": \"judge\",\n",
    "    },\n",
    ")\n",
    "workflow.add_conditional_edges(\n",
    "    \"judge\",\n",
    "    final_answer_router,\n",
    "    {\n",
    "        \"planning\": \"planning\",\n",
    "        \"end\": END,\n",
    "    }\n",
    ")\n",
    "\n",
    "# Compile the graph\n",
    "# 编译图\n",
    "app = workflow.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example usecase for PhD academic research\n",
    "\n",
    "This cell tests the workflow with several example queries. These queries are designed to evaluate the agent on the following aspects:\n",
    "- Completing tasks that are representative of the work a PhD researcher might need to perform.\n",
    "- Addressing more specific tasks that require researching papers within a defined timeframe.\n",
    "- Tackling tasks across multiple areas of research.\n",
    "- Critically evaluating its own responses by sourcing specific information from the papers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_inputs = [\n",
    "#     # 下载并总结这篇论文的发现\n",
    "#     \"下载并总结这篇论文的研究发现: https://pmc.ncbi.nlm.nih.gov/articles/PMC11379842/pdf/11671_2024_Article_4070.pdf\",\n",
    "\n",
    "#     # 查找量子机器学习相关论文\n",
    "#     \"能帮我找到8篇关于量子机器学习的论文吗?\",\n",
    "\n",
    "#     # 查找CRISPR最新研究论文\n",
    "#     \"\"\"查找2023-2024年关于CRISPR在治疗遗传疾病中应用的最新论文，\n",
    "#     重点关注临床试验和安全协议\"\"\",\n",
    "\n",
    "#     # 查找蛋白质折叠预测相关论文\n",
    "#     \"\"\"查找并分析2023-2024年关于Transformer架构在蛋白质折叠预测中应用的论文，\n",
    "#     特别关注那些有实验验证的新型架构修改\"\"\"\n",
    "# ]\n",
    "\n",
    "test_inputs = [\n",
    "    \"Download and summarize the findings of this paper: https://pmc.ncbi.nlm.nih.gov/articles/PMC11379842/pdf/11671_2024_Article_4070.pdf\",\n",
    "\n",
    "    \"Can you find 8 papers on quantum machine learning?\",\n",
    "\n",
    "    \"\"\"Find recent papers (2023-2024) about CRISPR applications in treating genetic disorders, \n",
    "    focusing on clinical trials and safety protocols\"\"\",\n",
    "\n",
    "    \"\"\"Find and analyze papers from 2023-2024 about the application of transformer architectures in protein folding prediction, \n",
    "    specifically looking for novel architectural modifications with experimental validation.\"\"\"\n",
    "]\n",
    "\n",
    "# Run tests and store the results for later visualisation\n",
    "outputs = []\n",
    "for test_input in test_inputs:\n",
    "    final_answer = await print_stream(app, test_input)\n",
    "    outputs.append(final_answer.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Display results\n",
    "\n",
    "This cell displays the results of the test queries for a more compact visualisation of the results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## Input:\n",
       "\n",
       "Download and summarize the findings of this paper: https://pmc.ncbi.nlm.nih.gov/articles/PMC11379842/pdf/11671_2024_Article_4070.pdf\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "## Output:\n",
       "\n",
       "The paper titled \"Advances, limitations and perspectives in the use of celecoxib-loaded nanocarriers in therapeutics of cancer\" reviews the development and potential of celecoxib (CXB)-loaded nanocarriers in cancer treatment. Celecoxib is a selective COX-2 inhibitor used in cancer therapy, but its use is limited by the need for high doses, which can cause severe side effects. Nanocarriers offer a promising solution by improving the drug's biopharmaceutical properties, allowing for controlled release and targeted delivery.\n",
       "\n",
       "### Key Findings:\n",
       "\n",
       "1. **Nanocarrier Types and Materials**: \n",
       "   - CXB-loaded nanocarriers are primarily based on polymers and lipids, using materials like poly(lactic-co-glycolic acid) (PLGA), cholesterol, phospholipids, and poly(ethylene glycol) (PEG).\n",
       "   - These carriers enhance drug solubility, stability, and bioavailability, and can be engineered for targeted delivery to tumor sites.\n",
       "\n",
       "2. **Advancements in Formulations**:\n",
       "   - Recent developments include the use of cell surface ligands, co-delivery of synergistic agents, and materials that provide imaging capabilities.\n",
       "   - The combination of CXB with other anti-inflammatory drugs or apoptosis inducers shows promise in enhancing therapeutic effects.\n",
       "\n",
       "3. **Clinical and Preclinical Studies**:\n",
       "   - The research is mostly in preclinical stages, with no current clinical trials using CXB-loaded nanocarriers for cancer treatment.\n",
       "   - In vivo studies have increased since 2017, indicating progress towards potential clinical applications.\n",
       "\n",
       "4. **Challenges and Future Directions**:\n",
       "   - The main challenges include CXB's low water solubility and the complexity of scaling up nanocarrier production for clinical use.\n",
       "   - Future research should focus on optimizing nanocarrier design for stability, targeting, and controlled release, as well as exploring synergistic drug combinations.\n",
       "\n",
       "5. **Potential Impact**:\n",
       "   - CXB-loaded nanocarriers could significantly enhance the efficacy of cancer treatments by improving drug delivery and reducing side effects.\n",
       "   - The ability of CXB to potentiate the effects of established chemotherapeutic agents is a major clinical advancement.\n",
       "\n",
       "The paper highlights the potential of nanotechnology to revolutionize cancer therapy by enabling more effective and less harmful treatment options through the use of CXB-loaded nanocarriers.\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "## Input:\n",
       "\n",
       "Can you find 8 papers on quantum machine learning?\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "## Output:\n",
       "\n",
       "Here are 8 papers on quantum machine learning:\n",
       "\n",
       "1. **Quantum Circuit Learning**\n",
       "   - **Authors**: Mitarai, Kosuke; Negoro, Makoto; Kitagawa, Masahiro; Fujii, Keisuke\n",
       "   - **Published Date**: April 23, 2019\n",
       "   - **Abstract**: This paper proposes a classical-quantum hybrid algorithm for machine learning on near-term quantum processors, called quantum circuit learning. The framework allows a quantum circuit to learn tasks by tuning parameters, circumventing high-depth circuits. Theoretical and numerical simulations show that a quantum circuit can approximate nonlinear functions.\n",
       "   - **URL**: [Quantum Circuit Learning](http://arxiv.org/abs/1803.00745)\n",
       "\n",
       "2. **Quantum Machine Learning**\n",
       "   - **Authors**: Biamonte, Jacob; Wittek, Peter; Pancotti, Nicola; Rebentrost, Patrick; Wiebe, Nathan; Lloyd, Seth\n",
       "   - **Published Date**: May 10, 2018\n",
       "   - **Abstract**: This paper explores the potential of quantum computers to outperform classical computers on machine learning tasks. It discusses the challenges and paths towards solutions in quantum machine learning.\n",
       "   - **URL**: [Quantum Machine Learning](http://arxiv.org/abs/1611.09347)\n",
       "\n",
       "3. **The Power of One Qubit in Machine Learning**\n",
       "   - **Authors**: Ghobadi, Roohollah; Oberoi, Jaspreet S.; Zahedinejhad, Ehsan\n",
       "   - **Published Date**: June 8, 2019\n",
       "   - **Abstract**: This paper proposes a kernel-based quantum machine learning algorithm that can be implemented on near-term quantum devices, using deterministic quantum computing with one qubit.\n",
       "   - **URL**: [The Power of One Qubit in Machine Learning](http://arxiv.org/abs/1905.01390)\n",
       "\n",
       "4. **Quantum Machine Learning: A Classical Perspective**\n",
       "   - **Authors**: Ciliberto, Carlo; Herbster, Mark; Ialongo, Alessandro Davide; Pontil, Massimiliano; Rocchetto, Andrea; Severini, Simone; Wossnig, Leonard\n",
       "   - **Published Date**: February 13, 2018\n",
       "   - **Abstract**: This review discusses the potential of quantum computation to speed up classical machine learning algorithms, highlighting the limitations and advantages of quantum resources for learning problems.\n",
       "   - **URL**: [Quantum Machine Learning: A Classical Perspective](http://arxiv.org/abs/1707.08561)\n",
       "\n",
       "5. **Quantum Machine Learning Over Infinite Dimensions**\n",
       "   - **Authors**: Lau, Hoi-Kwan; Pooser, Raphael; Siopsis, George; Weedbrook, Christian\n",
       "   - **Published Date**: November 14, 2016\n",
       "   - **Abstract**: This paper generalizes quantum machine learning to infinite-dimensional systems, presenting subroutines for quantum machine learning algorithms on continuous-variable quantum computers.\n",
       "   - **URL**: [Quantum Machine Learning Over Infinite Dimensions](http://arxiv.org/abs/1603.06222)\n",
       "\n",
       "6. **Experimental Demonstration of Quantum Learning Speed-up with Classical Input Data**\n",
       "   - **Authors**: Lee, Joong-Sung; Bang, Jeongho; Hong, Sunghyuk; Lee, Changhyoup; Seol, Kang Hee; Lee, Jinhyoung; Lee, Kwang-Geol\n",
       "   - **Published Date**: November 22, 2018\n",
       "   - **Abstract**: This paper demonstrates a quantum-classical hybrid machine learning approach, showing a quantum learning speed-up of approximately 36% compared to classical machines.\n",
       "   - **URL**: [Experimental Demonstration of Quantum Learning Speed-up](http://arxiv.org/abs/1706.01561)\n",
       "\n",
       "7. **Quantum-Enhanced Machine Learning**\n",
       "   - **Authors**: Dunjko, Vedran; Taylor, Jacob M.; Briegel, Hans J.\n",
       "   - **Published Date**: October 26, 2016\n",
       "   - **Abstract**: This work proposes a systematic approach to machine learning from the perspective of quantum information, covering supervised, unsupervised, and reinforcement learning.\n",
       "   - **URL**: [Quantum-Enhanced Machine Learning](http://arxiv.org/abs/1610.08251)\n",
       "\n",
       "8. **An Efficient Quantum Algorithm for Generative Machine Learning**\n",
       "   - **Authors**: Gao, Xun; Zhang, Zhengyu; Duan, Luming\n",
       "   - **Published Date**: November 6, 2017\n",
       "   - **Abstract**: This paper proposes a quantum algorithm for generative machine learning, showing exponential improvements in training and inference over classical algorithms.\n",
       "   - **URL**: [An Efficient Quantum Algorithm for Generative Machine Learning](http://arxiv.org/abs/1711.02038)\n",
       "\n",
       "These papers cover a range of topics within quantum machine learning, from theoretical frameworks to experimental demonstrations.\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "## Input:\n",
       "\n",
       "Find recent papers (2023-2024) about CRISPR applications in treating genetic disorders, \n",
       "    focusing on clinical trials and safety protocols\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "## Output:\n",
       "\n",
       "Here are some recent papers (2023-2024) on CRISPR applications in treating genetic disorders, focusing on clinical trials and safety protocols:\n",
       "\n",
       "1. **CRISPR-Cas9 Gene Editing Tool: Potential Treatment for Sickle Cell Disease**\n",
       "   - **Authors**: Young, Brittany\n",
       "   - **Published Date**: April 25, 2024\n",
       "   - **Abstract**: Not provided.\n",
       "   - **URL**: [Read the paper](https://digitalcommons.sacredheart.edu/cgi/viewcontent.cgi?article=2403&context=acadfest)\n",
       "\n",
       "2. **Progress and Harmonization of Gene Editing to Treat Human Diseases: Proceeding of COST Action CA21113 GenE-HumDi**\n",
       "   - **Authors**: Cavazza, Alessia; González Martínez, Coral; Sánchez Martín, Rosario María; Martín Molina, Francisco; Benabdellah, Karim; COST Action CA21113\n",
       "   - **Published Date**: December 12, 2023\n",
       "   - **Abstract**: This publication discusses the efforts of the GenE-HumDi network to expedite the application of genome editing for therapeutic purposes in treating human diseases. It covers aspects like enhancing genome editing technologies, assessing delivery systems, addressing safety concerns, promoting clinical translation, and developing regulatory guidelines.\n",
       "   - **URL**: [Read the paper](https://digibug.ugr.es/bitstream/10481/86007/1/1-s2.0-S2162253123002846-main.pdf)\n",
       "\n",
       "3. **Germline Genome Editing of Human IVF Embryos Should Not Be Subject to Overly Stringent Restrictions**\n",
       "   - **Authors**: Smith, Kevin\n",
       "   - **Published Date**: July 5, 2024\n",
       "   - **Abstract**: This paper critiques the restrictive criteria for germline genome editing, advocating for a balanced approach that weighs potential benefits against risks. It suggests that ethical oversight combined with genetic scrutiny can enable responsible use of the technology.\n",
       "   - **URL**: [Read the paper](https://rke.abertay.ac.uk/files/81349925/Smith_GermlineGenomeEditing_Publised_2024.pdf)\n",
       "\n",
       "4. **Balancing Progress and Ethics: Exploring the Science and Ethics of Gene Editing: Literature Review**\n",
       "   - **Authors**: Burgess, Jackson\n",
       "   - **Published Date**: January 1, 2024\n",
       "   - **Abstract**: Not provided.\n",
       "   - **URL**: [Read the paper](https://scholarworks.uni.edu/cgi/viewcontent.cgi?article=1929&context=hpt)\n",
       "\n",
       "These papers provide insights into the current state of CRISPR technology in clinical settings, focusing on safety protocols and ethical considerations.\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "## Input:\n",
       "\n",
       "Find and analyze papers from 2023-2024 about the application of transformer architectures in protein folding prediction, \n",
       "    specifically looking for novel architectural modifications with experimental validation.\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "## Output:\n",
       "\n",
       "I found several papers from 2023 that discuss the application of transformer architectures in protein folding prediction, with a focus on novel architectural modifications and experimental validation. Here are some of the most relevant papers:\n",
       "\n",
       "1. **Protein tertiary structure prediction and refinement using deep learning**\n",
       "   - **Authors**: Wu, Tianqi\n",
       "   - **Published Date**: 2023-01-08\n",
       "   - **Abstract**: This paper discusses the development of a method called TransPross, which applies a 1D transformer network and attention mechanism for protein secondary structure prediction. It also introduces ATOMRefine, a novel end-to-end protein structure refinement tool. The paper emphasizes the use of deep learning techniques in improving protein structure prediction.\n",
       "   - **URL**: [Read the paper](https://mospace.umsystem.edu/xmlui/bitstream/10355/94100/1/WuTianqiResearch.pdf)\n",
       "\n",
       "2. **Enhancing the Protein Tertiary Structure Prediction by Multiple Sequence Alignment Generation**\n",
       "   - **Authors**: Zhang, Le; Chen, Jiayang; Shen, Tao; Li, Yu; Sun, Siqi\n",
       "   - **Published Date**: 2023-06-02\n",
       "   - **Abstract**: This paper introduces MSA-Augmenter, a novel generative language model that uses protein-specific attention mechanisms to generate novel protein sequences. These sequences enhance the accuracy of structural property predictions, especially when existing sequences lack homologous families.\n",
       "   - **URL**: [Read the paper](http://arxiv.org/abs/2306.01824)\n",
       "\n",
       "3. **HelixFold-Single: MSA-free Protein Structure Prediction by Using Protein Language Model as an Alternative**\n",
       "   - **Authors**: Fang, Xiaomin; Wang, Fan; Liu, Lihang; He, Jingzhou; Lin, Dayong; Xiang, Yingfei; Zhang, Xiaonan; Wu, Hua; Li, Hui; Song, Le\n",
       "   - **Published Date**: 2023-02-21\n",
       "   - **Abstract**: This paper presents HelixFold-Single, which combines a large-scale protein language model with AlphaFold2's geometric learning capabilities. It aims to predict protein structures using only primary sequences, bypassing the need for multiple sequence alignments (MSAs).\n",
       "   - **URL**: [Read the paper](http://arxiv.org/abs/2207.13921)\n",
       "\n",
       "4. **Integration of persistent Laplacian and pre-trained transformer for protein solubility changes upon mutation**\n",
       "   - **Authors**: Wee, JunJie; Chen, Jiahui; Xia, Kelin; Wei, Guo-Wei\n",
       "   - **Published Date**: 2023-11-02\n",
       "   - **Abstract**: This work integrates persistent Laplacians and pre-trained Transformers to predict protein solubility changes upon mutation. The model outperforms existing methods and improves the state-of-the-art by up to 15%.\n",
       "   - **URL**: [Read the paper](http://arxiv.org/abs/2310.18760)\n",
       "\n",
       "These papers provide insights into the latest advancements in using transformer architectures for protein folding prediction, with a focus on novel modifications and experimental validation. You can explore these papers further to gain a deeper understanding of the specific architectural innovations and their experimental outcomes.\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for input, output in zip(test_inputs, outputs):\n",
    "    display(Markdown(f\"## Input:\\n\\n{input}\\n\\n\"))\n",
    "    display(Markdown(f\"## Output:\\n\\n{output}\\n\\n\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparative Analysis\n",
    "\n",
    "In this comprehensive analysis, we evaluated our Scientific Paper Agent against two leading AI knowledge co pilots : Microsoft Copilot and Perplexity AI. Using a standardized query - \"Find 8 papers on quantum machine learning\" - we conducted a detailed comparison across multiple dimensions to understand the strengths, limitations, and optimal use cases for each system.\n",
    "\n",
    "\n",
    "#### Test Case Implementation\n",
    "\n",
    "We implemented a controlled test using the same research query across all three platforms:\n",
    "- Query: \"Find 8 papers on quantum machine learning\"\n",
    "- Sample Size: Multiple test runs to ensure consistency\n",
    "- Evaluation Time: Early 2024\n",
    "- Metrics Tracked: Response time, metadata quality, and result structure\n",
    "\n",
    "#### Key Findings\n",
    "\n",
    "While our agent demonstrated superior academic rigor and metadata completeness, taking approximately 30 seconds per query, competitors like Microsoft Copilot (2 seconds) and Perplexity AI (4-5 seconds) showed advantages in response speed. This tradeoff between speed and depth reflects different design philosophies and target use cases.\n",
    "\n",
    "The comparative analysis reveals a clear differentiation in approaches:\n",
    "- Our Agent: Optimized for thorough academic research with comprehensive validation\n",
    "- Microsoft Copilot: Focused on rapid information retrieval and general overview\n",
    "- Perplexity AI: Balanced approach with emphasis on source verification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Microsoft  copilot results \n",
    "\n",
    "![image](https://i.ibb.co/y4Zf4Pc/Screenshot-2024-11-17-at-21-40-21.png)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perplexity AI results\n",
    "![image](https://i.ibb.co/n1rr7kW/Screenshot-2024-11-17-at-21-40-42.png)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics Comparsion\n",
    "![image](https://i.ibb.co/5KbTmFq/Screenshot-2024-11-17-at-22-03-43.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we present a comprehensive comparison between our research assistant agent and leading platforms (Microsoft Copilot and Perplexity AI). Using a standardized query - \"Find 8 papers on quantum machine learning\" - we evaluated performance across key metrics including response time, metadata quality, and academic value. Our analysis reveals distinct trade-offs: while our agent takes longer to process (30s vs. 2-5s), it provides significantly more detailed metadata, validated sources, and structured academic output. The comparison table above breaks down these differences across multiple dimensions, helping users choose the right tool for their specific research needs - whether it's quick exploration (where Copilot excels) or deep academic research (where our agent shows its strengths)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "##  Limitations\n",
    "\n",
    "1. Technical Limitations\n",
    "    - API rate limits for paper access\n",
    "    - Handle time for large PDFs\n",
    "    - Limited to publicly accessible papers\n",
    "  \n",
    "2. Functional Limitations\n",
    "    - No support for image analysis in papers\n",
    "    - Limited context window for very long papers\n",
    "    - Cannot perform mathematical derivations\n",
    "    - Language constraints for non-English papers\n",
    "\n",
    "\n",
    "## Potential Improvements:\n",
    "\n",
    "1. Technical Improvements\n",
    "    - Implement parallel processing for multiple papers\n",
    "    - Add caching system for frequently accessed papers\n",
    "    - Integrate multiple academic APIs for broader coverage\n",
    "    - Implement batch processing for large datasets\n",
    "\n",
    "2. Functional Improvements\n",
    "    - Add support for figure and table extraction\n",
    "    - Implement cross-referencing between papers\n",
    "    - Add citation network analysis\n",
    "    - Include domain-specific validation rules\n",
    "        \n",
    "3. User Experience\n",
    "    - Add interactive feedback mechanisms\n",
    "    - Implement progress tracking\n",
    "    - Add customizable validation criteria\n",
    "    - Include export options for research summaries\n",
    "        \n",
    "   \n",
    "## Specific Use Cases:\n",
    "\n",
    "1. Academic Research, Literature review and paper analysis.\n",
    "    - Comprehensive search\n",
    "    - Citation tracking\n",
    "    - Cross-reference validation\n",
    "\n",
    "2. Industry Research, Technical documentation and patent analysis.\n",
    "    - Focused search\n",
    "    - Technical specification extraction\n",
    "    - Competitive analysis\n",
    "\n",
    "3. Educational, Student research assistance.\n",
    "    - Simplified explanations\n",
    "    - Learning resource identification\n",
    "    - Guided research process\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This implementation demonstrates how state-driven architectures can transform academic paper analysis. By combining LangGraph's orchestration capabilities with robust API integrations, we've created a system that maintains research rigor while automating key aspects of paper processing. The workflow's emphasis on validation and quality control ensures reliable research outputs while significantly streamlining the paper analysis process.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
